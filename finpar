#!/usr/bin/env python

import os
import subprocess
import json

def executable(fpath):
    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

class Implementation:
    def __init__(self, finpar_lib_dir, benchmark_lib_dir, name, directory):
        if os.path.isdir(directory):
            self.directory = directory
        else:
            if os.path.isfile(directory):
                raise Exception("File %s exists, but should be an implementation directory" % directory)
            else:
                raise Exception("Implementation directory %s does not exist " % directory)

        self.directory = directory
        self.name = name
        self.finpar_lib_dir = finpar_lib_dir
        self.benchmark_lib_dir = benchmark_lib_dir

        runfile = os.path.join(self.directory, "run")
        instantiatefile = os.path.join(self.directory, "instantiate")
        if not executable(runfile):
            raise Exception("%s is not an executable file" % runfile)
        if not executable(instantiatefile):
            raise Exception("%s is not an executable file" % instantiatefile)

    def instantiate(self, where, input_data):
        if not os.path.isdir(where):
            os.mkdir(where)

        # We use the current environment as a template, and then
        # extend it with some new variables.
        instantiate_env = os.environ.copy()
        instantiate_env["FINPAR_LIB_DIR"] = self.finpar_lib_dir
        instantiate_env["FINPAR_BENCHMARK_LIB_DIR"] = self.benchmark_lib_dir
        instantiate_env["FINPAR_IMPLEMENTATION"] = self.directory
        instantiate_env["FINPAR_INPUT"] = input_data

        proc = subprocess.Popen(os.path.join(self.directory, "instantiate"),
                                env=instantiate_env,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                close_fds=True,
                                cwd=where)
        (stdoutdata, stderrdata) = proc.communicate(input=None)
        return (proc.returncode, stdoutdata, stderrdata)

class Benchmark:
    # In the constructor, we do some basic sanity-checking.  The
    # primary purpose here is to provide better error messages than if
    # we fail to read files later down the line.
    def __init__(self, finpardir, name, directory):
        self.finpardir = finpardir
        self.name = name

        if os.path.isdir(directory):
            self.directory = directory
        else:
            raise Exception("Benchmark directory %s does not exist" % directory)
        dircontents = os.listdir(self.directory)

        # The lib directories do not have to exist.
        self.benchmark_lib_dir = os.path.join(self.directory, "lib")
        self.finpar_lib_dir = os.path.join(self.finpardir, "lib")

        if "implementations" in dircontents:
            self.__gather_implementations()
        else:
            raise Exception("No implementation directory in %s" % directory)

        if "datasets" in dircontents:
            self.datasets = os.path.join(self.directory, "datasets")
            self.__gather_datasets()
        else:
            raise Exception("No datasets directory in %s" % directory)

    def __gather_implementations(self):
        self.implementations = []
        for impl in os.listdir(os.path.join(self.directory, "implementations")):
            implpath = os.path.join(self.directory, "implementations", impl)
            if os.path.isdir(implpath):
                self.implementations.append(Implementation(self.finpar_lib_dir,
                                                           self.benchmark_lib_dir,
                                                           impl,
                                                           os.path.join(self.directory, "implementations", impl)))

    def __gather_datasets(self):
        self.datasets = {}
        for dataset in os.listdir(os.path.join(self.directory, "datasets")):
            datasetpath = os.path.join(self.directory, "datasets", dataset)
            if os.path.isdir(datasetpath):
                inputfile = os.path.join(self.directory, "datasets", dataset, "input.json")
                outputfile = os.path.join(self.directory, "datasets", dataset, "output.json")
                if not os.path.isfile(inputfile):
                    raise Exception("File %s missing or not a file." % inputfile)
                if not os.path.isfile(outputfile):
                    raise Exception("File %s missing or not a file." % outputfile)

                self.datasets[dataset] = { 'input_file' : inputfile,
                                           'expected_output' : read_json_file(outputfile)
                                         }

def note_result(results, benchmark, implementation, dataset, runtime):
    if type(runtime) is int:
        status = str(runtime) + "ms"
    else:
        status = runtime
    print("Benchmark %s, implementation %s, dataset %s: %s" %
          (benchmark, implementation, dataset, status))
    results[(benchmark,implementation,dataset)] = runtime

def run_all_benchmark_implementations(finpar_dir, workdir, benchmarkdir):
    b = Benchmark(finpar_dir, os.path.basename(benchmarkdir), benchmarkdir)
    insts = []
    results = {}

    for impl in b.implementations:
        for dataset in b.datasets:
            instdir = os.path.join(workdir,
                                   "%s-%s-%s" % (b.name, impl.name, dataset))
            print("Instantiating in %s" % instdir)
            (retcode, stdout, stderr) = impl.instantiate(instdir, b.datasets[dataset]['input_file'])
            if retcode != 0:
                print("Instantiation failed with exit code %d.\nStdout:\n%s\nStderr:\n%s" %
                      (retcode, stdout, stderr))
            else:
                insts.append({ 'benchmark' : b.name,
                               'implementation' : impl.name,
                               'dataset' : dataset,
                               'instantiation_directory' : instdir,
                               'expected_output' : b.datasets[dataset]['expected_output']
                           })

    for inst in insts:
        instdir = inst['instantiation_directory']
        print("\nRunning %s" % instdir)
        stdoutpath = os.path.join(instdir, "stdout.log")
        stderrpath = os.path.join(instdir, "stderr.log")
        with open(stdoutpath, "w") as stdout:
            with open(stderrpath, "w") as stderr:
                print("Output can be found in %s and %s" % (stdoutpath, stderrpath))
                proc = subprocess.Popen(os.path.join(instdir, "run"),
                                        stdout=stdout,
                                        stderr=stderr,
                                        stdin=subprocess.PIPE,
                                        close_fds=True,
                                        cwd=instdir)
                proc.stdin.close() # Make sure stdin EOFs.
                proc.wait()

                if proc.returncode == 0:
                    success = "success"
                else:
                    success = "failure"

                try:
                    runtime = read_runtime(instdir)
                except IOError:
                    note_result(results,
                                inst['benchmark'],
                                inst['implementation'],
                                inst['dataset'],
                                "no runtime measurement")
                    continue
                except ValueError:
                    note_result(results,
                                inst['benchmark'],
                                inst['implementation'],
                                inst['dataset'],
                                "non-integer runtime measurement")
                    continue

                try:
                    result = read_json_file(os.path.join(instdir, "result.json"))
                    expected = inst['expected_output']
                    if compare_json(result, expected):
                        note_result(results,
                                    inst['benchmark'],
                                    inst['implementation'],
                                    inst['dataset'],
                                    runtime)
                    else:
                        note_result(results,
                                    inst['benchmark'],
                                    inst['implementation'],
                                    inst['dataset'],
                                    "invalid result")
                except (IOError, ValueError):
                    note_result(results,
                                inst['benchmark'],
                                inst['implementation'],
                                inst['dataset'],
                                "cannot read result")

    return results

def read_runtime(instdir):
    with open(os.path.join(instdir, "runtime.txt"), "r") as file:
        return int(file.read())

def read_json_file(filename):
    with open(filename, "r") as file:
        return json.loads(file.read())

epsilon = 0.001

# Generic comparison function using epsilon for comparing floats.
def compare_json(json1, json2):
    if type(json1) != type(json2):
        return False

    if type(json1) is float:
        return abs(json1-json2) < epsilon

    if type(json1) is list:
        if len(json1) != len(json2):
            return False
        for x,y in zip(json1, json2):
            if not compare_json(x,y):
                return False
        return True

    if type(json1) is dict:
        keys1 = json1.keys()
        keys2 = json2.keys()
        keys1.sort()
        keys2.sort()
        if keys1 == keys2:
            for key in keys1:
                if not compare_json(json1[key], json2[key]):
                    return False
            return True
        else:
            return False

    return json1 == json2

def test():
    run_all_benchmark_implementations('/home/athas/repos/finpar',
                                      '/home/athas/repos/finpar/instantiations',
                                      '/home/athas/repos/finpar/benchmarks/CalibVolDiff')
